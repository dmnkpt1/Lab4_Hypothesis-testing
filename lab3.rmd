---
title: 'P&S-2025: Lab assignment 3'
author: "Dominika Petrus, Anastasia Shopska, Stanistav Kovalenko"
output:
  html_document:
    df_print: paged
---

**Work breakdown**

-   Task 1: Anastasia Shopska

-   Task 2: Anastasia Shopska

-   Task 3: Dominika Petrus

-   Task 4:

Data was generated as:

## Problem 1

$$
H_o : \mu\_1 = \mu\_2 \ \text{vs.} \  H_1 : \mu\_1 \ne \mu\_2;\  \sigma^2_1 = \sigma^2_2 = 1
$$

```{r}
TEAM_ID <- 16
ALPHA <- 0.05
N1 <- 100
N2 <- 50

calculate_ak <- function(k, n) {
  value <- k * log(k^2 * n + pi)
  return(value - floor(value))
}

k_indices_X <- 1:N1
k_indices_Y <- (N1 + 1):(N1 + N2)

a_k_X <- sapply(k_indices_X, calculate_ak, n = TEAM_ID)
a_k_Y <- sapply(k_indices_Y, calculate_ak, n = TEAM_ID)

X <- qnorm(a_k_X)
Y <- qnorm(a_k_Y)

mean_X <- mean(X)
var_X <- var(X)
std_X <- sd(X)
mean_Y <- mean(Y)
var_Y <- var(Y)
std_Y <- sd(Y)

cat(sprintf("Sample X (N1=%d):\n", N1))
cat(sprintf("Sample Mean (X̄): %.4f\n", mean_X))
cat(sprintf("Sample Std Dev (S1): %.4f (S1^2: %.4f)\n", std_X, var_X))
cat(sprintf("Sample Y (N2=%d):\n", N2))
cat(sprintf("Sample Mean (Ȳ): %.4f\n", mean_Y))
cat(sprintf("Sample Std Dev (S2): %.4f (S2^2: %.4f)\n", std_Y, var_Y))


sigma_squared <- 1
Z_statistic <- (mean_X - mean_Y) / sqrt(sigma_squared/N1 + sigma_squared/N2)

Z_critical <- qnorm(1 - ALPHA / 2)

p_value_Z <- 2 * (1 - pnorm(abs(Z_statistic)))

reject_H0_Z <- p_value_Z < ALPHA

cat(sprintf("\n1. Test Used: Z-test for the difference in means with known variances.\n"))
cat(sprintf("2. Rejection Region for H0 (at %.2f level): |Z| > %.4f (Two-sided).\n", ALPHA, Z_critical))
cat(sprintf("3. Calculated Test Statistic Z: %.4f\n", Z_statistic))
cat(sprintf("4. p-value: %.4f\n", p_value_Z))
cat(sprintf("5. Conclusion: "))
cat(sprintf("Since p-value (%.4f) %s α (%.2f), we %s H0.\n", 
            p_value_Z, 
            ifelse(reject_H0_Z, "<", ">="), 
            ALPHA, 
            ifelse(reject_H0_Z, "REJECT", "DO NOT REJECT")))


```

## Problem 2

$$
H_o : \sigma_1 = \sigma_2 \ \text{vs.}\  H_1 : \sigma_1 > \sigma_2 \ \mu_1 = \mu_2 \ is \ known
$$

```{r}
df1 <- N1 - 1 # Numerator degrees of freedom
df2 <- N2 - 1 # Denominator degrees of freedom

F_statistic <- var_X / var_Y

# Rejection Region
F_critical <- qf(1 - ALPHA, df1 = df1, df2 = df2)

# Calculate p-value
p_value_F <- 1 - pf(F_statistic, df1 = df1, df2 = df2)

# Conclusion
reject_H0_F <- p_value_F < ALPHA

cat(sprintf("\n1. Test Used: F-test for variances (F-distribution).\n"))
cat(sprintf("2. Rejection Region for H0 (at %.2f level): F > %.4f (Right-sided).\n", ALPHA, F_critical))
cat(sprintf("3. Calculated Test Statistic F = %.4f\n", F_statistic))
cat(sprintf("4. p-value: %.4f\n", p_value_F))
cat(sprintf("5. Conclusion: "))
cat(sprintf("Since p-value (%.4f) %s α (%.2f), we %s H0.\n", 
            p_value_F, 
            ifelse(reject_H0_F, "<", ">="), 
            ALPHA, 
            ifelse(reject_H0_F, "REJECT", "DO NOT REJECT")))
```

## Problem 3. Kolmogorov–Smirnov test



### The main idea behind the KS test

If we have data with unknown distribution we can test if our data fits to some known distribution (like normal, exp ..) using Kolmogorov’s Goodness-of-fit test (for 1 sample) or KS (for 2 sample) test, Kolmogorov’s Goodness-of-fit test use the fact that by LLN the empirical c.d.f. is close to theoretical . And assume these hypotheses: $$
H_o : F = F_0 \ vs \ H_1 : F \ne F_0
$$

where F is distribution of our data and $F_0$ hypothetical, and test statistic as: $$
d := \sup_{t \in \mathbb{R}} \left| \widehat{F}_x(t) - F_0(t) \right|
$$ And rejection region: $$C_\alpha = \{x \in \mathbb{R}^n |\ d \ge d^n_{1-\alpha} \}$$

While Kolmogorov-Smirnov we use to test if 2 samples have same distribution, it differs from previous in a way that we use max difference between two e.c.d.f with statistic: $$
d := \sup_{t \in \mathbb{R}} \left| \widehat{F}_x(t) - \widehat{F}_y(t) \right|
$$

and hypotheses:

$$
H_o : F_x = F_y \ vs \ H_1 : F_x \ne F_y
$$


## Part a

In this part (and next one) we will apply Kolmogorov’s Goodness-of-fit test.

$\{x_k\}^{100}_{k=1}$ are normally distributed (with parameters calculated from the sample)

```{r}

x_data <- X
y_data <- Y
mean_of_a <- mean(x_data)
variance_of_a <- var(x_data)

test_x_is_normal <- ks.test(x_data, "pnorm", mean = mean_of_a, sd = sqrt(variance_of_a))

cat("Results of testing if our data are normally distributed", "\n")
print(test_x_is_normal)

```


Our p-value is equal to 0.9774, this mean that we cannot reject $H_0$ because our be value is bigger than 0.05 and that our data is distributed almost normal. We can see it on a plot:

```{r}
hist(x_data, freq = FALSE, col = "pink", ylim = c(0, 0.5))

curve(dnorm(x, mean = mean(x_data), sd = sd(x_data)), 
      col = "blue", add = TRUE)
```

## Part b

$\{|x_k|\}^{100}_{k=1}$ are exponentially distributed with $\lambda$ = 1

The difference now is that we take module of our data and test if it is exponentially distributed

```{r}

data_x_in_module <- abs(x_data)
lambda <- 1

test_module_x_is_exponential <- ks.test(x_data, "pexp", rate = lambda)
cat("Results of testing if data in module are exponentially distributed with rate of", "\n")
print(test_module_x_is_exponential)
```


Our p-value is equal to 2.2e-16, and this is very small value of p and means that we need to reject $H_0$ and our data is not exponentially distributed. Here is a plot:

```{r}
hist(data_x_in_module, freq = FALSE, breaks = 15, col = "purple",)

curve(dexp(x, rate = 1), 
      col = "blue", add = TRUE)
```

After observing it, it looks strange that we reject the null hypothesis, but we can see that distribution of our data falls faster (in the tail) while exponential stays over it

## Part c

$\{x_k\}^{100}_{k=1}$ and $\{y_l\}^{50}_{l=1}$ have the same distributions

(Here we need to apply Kolmogorov-Smirnov test because we have two samples)

```{r}

test_x_same_as_y <- ks.test(x_data, y_data)

cat("Results of testing if x has same distribution as y", "\n")
print(test_x_same_as_y)

```

We can see that p-value for this case is equal to 0.3503 what mean that we do not reject $H_0$ and our samples have same distribution. And here is a plot where we see that null hyp. is actually true

```{r}
plot(ecdf(x_data), col = "pink")
plot(ecdf(y_data), col = "blue",  add = TRUE)
```
## Problem 4
